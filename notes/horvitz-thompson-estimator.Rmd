
## Horvitz-Thompson estimator

********************************************************************************

This is a method for estimating the **mean** and **total** of a population of finite size $N$, from a sample of size $n$, where different individuals in the population have different probabilities of being included in the sample but the same expected value.

The true value of the population total ($\tau$) and the population mean ($\mu$) are,

$$ \tau = \sum_{i=1}^N y_i  \qquad\qquad  \mu = \frac{1}{N} \sum_{i=1}^N y_i $$

where,

$y_i$ is the value of individual $i$

$\pi_i$ is the **inclusion probablilty** for individual $i$ (the probability that individual $i$ is in the sample)

The Horvitz-Thompson estimators are,

$$ \widehat{\tau} = \sum_{i=1}^n \frac{y_i}{\pi_i}  \qquad\qquad  \widehat{\mu} = \frac{1}{N} \sum_{i=1}^n \frac{y_i}{\pi_i}  $$

So the higher the probability that individual $i$ is in the sample, the less weight the response $y_i$ is given when estimating the total.

********************************************************************************

### Expected value

To work this out we need to re-express the estimator as a function of the indicator variable, $z_i$,

$$ \widehat{\tau} = \sum_{i=1}^N z_i \frac{y_i}{\pi_i} $$

where,

$$
z_i = \begin{cases}
1 & \mbox{if the $i$th individual is in the sample} \\
0 & \mbox{otherwise}
\end{cases}
$$

The indicator variable follows the <a href="Bernoulli distribution.html">Bernoulli distribution</a> with probability $\pi_i$, so the expected value of $z_i$ is,

$$ \mbox{E}\left[ z_i \right] = \pi_i $$

The <a href="Expected value.html">expected value</a> of the estimator is therefore given by,

$$ 
\mbox{E}\left[ \widehat{\tau} \right] 
= \mbox{E}\left[ \sum_{i=1}^N z_i \frac{y_i}{\pi_i} \right] 
= \sum_{i=1}^N \mbox{E}\left[ z_i \frac{y_i}{\pi_i} \right] 
= \sum_{i=1}^N \frac{y_i}{\pi_i} \mbox{E}\left[ z_i \right] 
= \sum_{i=1}^N \frac{y_i}{\pi_i} \pi_i 
= \sum_{i=1}^N y_i 
= \tau 
$$

which shows that it is unbiased.

********************************************************************************

### Variance

For this we also need the variance of $z_i$,

$$ \mbox{Var}\left[ z_i \right] = \pi_i(1-\pi_i) $$

and the <a href="Covariance.html">covariance</a> of $z_i$ and $z_j$,

$$ \mbox{Cov}\left( z_i, z_j \right) = \mbox{E}\left[ z_i z_j \right] - \mbox{E}\left[ z_i \right] \mbox{E}\left[ z_j \right] = \pi_{ij} - \pi_i \pi_j $$

where $\pi_{ij}$ is the probability that individuals $i$ an $j$ are both in the sample.

The <a href="Variance.html">variance</a> of the estimator is therefore given by,

$$ 
\begin{align}
   \mbox{Var}\left[ \widehat{\tau} \right] 
&= \mbox{Var}\left[ \sum_{i=1}^N z_i \frac{y_i}{\pi_i} \right] \\
&= \sum_{i=1}^N \left( \frac{y_i}{\pi_i} \right)^2 \mbox{Var}\left[ z_i \right]
   + \sum_{i=1}^N \sum_{j>i}^N \left( \frac{y_i}{\pi_i} \frac{y_j}{\pi_j} \right) \mbox{Cov}\left[ z_i, z_j \right] \\
&= \sum_{i=1}^N \left( \frac{y_i}{\pi_i} \right)^2 \pi_i(1-\pi_i)
   + \sum_{i=1}^N \sum_{j>i}^N \left( \frac{y_i}{\pi_i} \frac{y_j}{\pi_j} \right) \left( \pi_{ij} - \pi_i \pi_j \right) \\
&= \sum_{i=1}^N \frac{ (1-\pi_i) }{ \pi_i } y_i^2
   + \sum_{i=1}^N \sum_{j>i}^N \frac{ \left( \pi_{ij} - \pi_i \pi_j \right) }{\pi_i \pi_j} y_i y_j
\end{align}
$$

********************************************************************************
